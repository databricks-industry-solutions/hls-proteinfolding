{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da7f92ff-f55e-4767-8f35-39f0ce611888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Install protein folding models\n",
    "Runs on serverless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f612de86-34d0-4452-9d8b-fae2c558265a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyyaml\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9018222-f7d3-4479-9ba1-e5f2f204366c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS protein_folding\")\n",
    "for model in ['alphafold','proteinmpnn','boltz','esmfold','rfdiffusion']:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS protein_folding.{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc8d31e-a108-47ba-bdb3-36048cfb2553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# these are large, but are required for AF2\n",
    "# If true, download all datasets required - but, we do not include download \n",
    "# of the full BFD even if set to True, opting to use bfd_small only for that\n",
    "# dataset expecting only very minor performance degradation with much faster inference.\n",
    "download_af2_datasets = False\n",
    "email = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b09f64c-55ba-4753-ab20-ecac5eb01d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "directory_path = '/'.join(notebook_path.split('/')[:-1])\n",
    "\n",
    "if download_af2_datasets:\n",
    "    default_yaml_path = \"/Workspace\"+directory_path+\"/install_incl_af2downloads.yaml\"\n",
    "else:\n",
    "    default_yaml_path = \"/Workspace\"+directory_path+\"/install.yaml\"\n",
    "\n",
    "base_path = str(Path(\"/Workspace\"+directory_path+\"/../tutorials\").resolve())\n",
    "\n",
    "print(default_yaml_path)\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a7e637-a87f-4104-ae3d-999863c25056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "with open(default_yaml_path, 'r') as file:\n",
    "    yaml_content = file.read()\n",
    "\n",
    "updated_yaml_content = re.sub(r'<email>', email, yaml_content)\n",
    "updated_yaml_content = re.sub(r'<root_path>', base_path, updated_yaml_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "761ba7ca-cdeb-4317-9fe0-0c15654b48f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### note to self:\n",
    "  - also Azure compute vs aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15300cd4-30ef-4f78-9e35-65a339b0a39a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import JobSettings\n",
    "from typing import Optional\n",
    "import yaml\n",
    "def create_job_from_yaml(yaml_path: Optional[str] = None, yaml_str: Optional[str] = None):\n",
    "    if yaml_path is not None:\n",
    "        with open(yaml_path) as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    elif yaml_str is not None:\n",
    "        config = yaml.safe_load(yaml_str)\n",
    "    else:\n",
    "        raise ValueError(\"Either yaml_path or yaml_str must be provided\")\n",
    "\n",
    "    outer_name = [k for k in config['resources']['jobs'].keys()][0]\n",
    "\n",
    "    # Full job settings deserialization\n",
    "    job_settings = JobSettings.from_dict(config['resources']['jobs'][outer_name])\n",
    "\n",
    "    # instantiate the client\n",
    "    w = WorkspaceClient()\n",
    "    \n",
    "    # create a job just with name\n",
    "    creation_info = w.jobs.create(name='new created job')\n",
    "    # now use job settings object to populate\n",
    "    w.jobs.reset(\n",
    "        job_id=creation_info.job_id,\n",
    "        new_settings=job_settings,\n",
    "    )\n",
    "    return creation_info.job_id\n",
    "\n",
    "try:\n",
    "    job_id = create_job_from_yaml(yaml_str = updated_yaml_content)\n",
    "    print(f\"Created job {job_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Job creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed11bd0-ad64-4332-b86a-a7ef99af6aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Run the workflow we just made\n",
    " - this will actually download model weights, create registered models and serve them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca406ce7-f38e-47a4-bb52-9fc8c774b661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w = WorkspaceClient()\n",
    "run_by_id = w.jobs.run_now(job_id=job_id).result()\n",
    "\n",
    "# could use and_wait on run_now_and_wait but may just timeout due to taking a while, consider...\n",
    "# clean up the no longer needed job once complete\n",
    "# w.jobs.delete(job_id=job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5742d0f-e41f-4599-88aa-83b10782d364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Now make the AF2 workflow ready to be run as needed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e66bb2b2-cd9d-4b1a-8cd2-5f58efe2f956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "default_yaml_path = \"/Workspace\"+directory_path+\"../tutorials/alphafold\"+\"/workflow/resources/example_workflow_setup.yaml\"\n",
    "af_notebooks_path = \"/Workspace\"+directory_path+\"../tutorials/alphafold\"+\"/workflow/notebooks\"\n",
    "\n",
    "# For Azure\n",
    "fold_compute = \"Standard_NC4as_T4_v3\"\n",
    "featurize_compute = \"Standard_F8\"\n",
    "\n",
    "with open(default_yaml_path, 'r') as file:\n",
    "    yaml_content = file.read()\n",
    "\n",
    "updated_yaml_content = re.sub(r'<email>', email, yaml_content)\n",
    "updated_yaml_content = re.sub(r'<notebooks_path>', af_notebooks_path, updated_yaml_content)\n",
    "updated_yaml_content = re.sub(r'<fold_compute>', fold_compute, updated_yaml_content)\n",
    "updated_yaml_content = re.sub(r'<featurize_compute>', featurize_compute, updated_yaml_content)\n",
    "\n",
    "try:\n",
    "    job_id = create_job_from_yaml(yaml_str = updated_yaml_content)\n",
    "    print(f\"Created job {job_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Job creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1509635-5ddb-41e9-836d-153a88cd7cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bc617f5-4a1e-420a-9f3d-9d6464165be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c406041c-150d-4e43-bce2-9efc7c44c007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "js = get_jobsettings(yaml_str=updated_yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c9e712-a802-42a3-94d9-d4dcba568838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "js.tasks[0].task_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bb1d325-1b6b-45dc-8318-a921407ad780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13e26268-f9fe-4cd5-ac70-13b6f8188441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6888821-193b-4a8f-94a1-658f88ebe6e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import JobSettings\n",
    "from typing import Optional\n",
    "import yaml\n",
    "\n",
    "def create_job_from_yaml(yaml_path: Optional[str] = None, yaml_str: Optional[str] = None):\n",
    "    if yaml_path is not None:\n",
    "        with open(yaml_path) as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    elif yaml_str is not None:\n",
    "        config = yaml.safe_load(yaml_str)\n",
    "    else:\n",
    "        raise ValueError(\"Either yaml_path or yaml_str must be provided\")\n",
    "    \n",
    "    outer_name = [k for k in config['resources']['jobs'].keys()][0]\n",
    "  \n",
    "    # Full job settings deserialization\n",
    "    job_settings = JobSettings.from_dict(config['resources']['jobs'][outer_name])\n",
    "    \n",
    "    # instantiate the client\n",
    "    w = WorkspaceClient()\n",
    "    \n",
    "    # create a job just with name\n",
    "    creation_info = w.jobs.create(name='new created job')\n",
    "    # now use job settings object to populate\n",
    "    w.jobs.reset(\n",
    "        job_id=creation_info.job_id,\n",
    "        new_settings=job_settings,\n",
    "    )\n",
    "    return creation_info.job_id\n",
    "\n",
    "try:\n",
    "    job_id = create_job_from_yaml(yaml_str = updated_yaml_content)\n",
    "    print(f\"Created job {job_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Job creation failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "install",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
