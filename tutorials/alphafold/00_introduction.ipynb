{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11143627-c2ec-48a6-99a6-dad4af1d2900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Running Alphafold\n",
    "\n",
    "Because Alphafold runs both expensive CPU and GPU jobs, we split up the two sets of tasks (featurization and folding).\n",
    "\n",
    "The python module at workflow/scripts/run_alphafold_split.py has flags to allow you to run just the featurization (do on a CPU machine), \n",
    "or to fold (using GPU) after loading up precomputed feature pickle. \n",
    "\n",
    "This setup means featurization and folding can be set as two tasks in a Databricks job, with each task running on appropriate compute.\n",
    "\n",
    "![alphafold_cpu_gpu](./static/alphafold_CPU_GPU_split.png)\n",
    "\n",
    "![alphafold_on_dbrx](./static/alphafold_dbrx_setup.png)\n",
    "\n",
    "Note that we suggest here manually placing yaml description (auto-generated for you) of the job in the workflows UI to create the job - with the yaml one can also set up a Databricks Asset Bundle instead to build these resources.\n",
    "\n",
    "### To setup Alphafold:\n",
    "\n",
    " 1. If not already downloaded in your workspace - download datasets\n",
    "    - Use 00_create_downloads_workflow to create a yaml that can be used to make a new workflows job\n",
    "      - In the workflows UI, click \"create job\", then in the upper right click the kebab menu and select \"switch to code version (yaml)\"\n",
    "    - Create the job using the yaml, and run it\n",
    "    - This runs, in parallel, the downloads (will take ~12 hours for longest download, most are in the few hours range) \n",
    "\n",
    " 2. Use 01_create_alphafold_workflow to build a workflow to actually run Alphafold\n",
    "    - creates a yaml you can use to make a new job\n",
    "    - create the workflows job\n",
    "    - Note, we set concurrency to 5, meaning 5 users can have jobs running at once and otherwise a queue is formed\n",
    "       - you can change this behavior at any time in the UI\n",
    " \n",
    " 3. Use the 02_run_alphafold_workflow here to set off a new run \n",
    "    - or, in the workflows UI page, hit arrow next to \"run\", and select run with different parameters\n",
    "\n",
    "![alphafold_on_wf](./static/af2_wf_screenshot.png)\n",
    "\n",
    "#### Optionally, you can use the notebooks to run Alphafold ad-hoc:\n",
    "\n",
    " - Run nb_run_af_featurize notebook on a CPU cluster\n",
    "   - Make sure you use conda with the licensing terms of Anaconda (here we do not use defaults)\n",
    "   - enter the protein and name the run as widgets in the notebook.\n",
    "\n",
    " - Run nb_run_af_fold notebook\n",
    "   - Do this on a GPU cluster (will work on CPU but GPU is much faster)\n",
    "   - re-enter your protein and run_name in widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c92570fd-939f-4ae2-b5f6-f2f4dfd3f5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_introduction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
